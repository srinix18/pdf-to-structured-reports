"""
Main orchestration script for the PDF processing pipeline.
"""
import logging
import sys
import json
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime
from tqdm import tqdm

from config.config import DATA_DIR, LOGS_DIR, LOG_FORMAT, LOG_LEVEL
from pipeline.detect_pdf_type import detect_pdf_type, get_pdf_info
from pipeline.extract_text import extract_text
from pipeline.clean_text import clean_pages
from pipeline.export_outputs import export_to_docx, create_output_directory
from pipeline.section_boundary_detector import SectionBoundaryDetector
from pipeline.section_content_extractor import extract_sections_from_pdf
from pipeline.section_hierarchy_builder import SectionHierarchyBuilder


def setup_logging(log_file: Path = None) -> None:
    """
    Configure logging for the pipeline.
    
    Args:
        log_file: Optional path to log file
    """
    handlers = [logging.StreamHandler(sys.stdout)]
    
    if log_file:
        log_file.parent.mkdir(parents=True, exist_ok=True)
        handlers.append(logging.FileHandler(log_file))
    
    logging.basicConfig(
        level=getattr(logging, LOG_LEVEL),
        format=LOG_FORMAT,
        handlers=handlers
    )


def parse_company_year(pdf_path: Path) -> tuple[str, str]:
    """
    Extract company name and year from directory structure or filename.
    
    Args:
        pdf_path: Path to PDF file
        
    Returns:
        Tuple of (company_name, year)
    """
    # Try to extract from directory structure
    # Expected: data/CompanyName/2022/report.pdf or data/CompanyName/report_2022.pdf
    
    parts = pdf_path.parts
    
    # Look for year in path (4-digit number between 1900-2099)
    year = None
    for part in reversed(parts):
        if part.isdigit() and len(part) == 4 and 1900 <= int(part) <= 2099:
            year = part
            break
    
    # If not in path, try filename
    if not year:
        import re
        year_match = re.search(r'(19|20)\d{2}', pdf_path.stem)
        if year_match:
            year = year_match.group(0)
        else:
            year = "unknown"
    
    # Get company name from parent directory or filename
    if pdf_path.parent.name.isdigit():
        # Year is directory, company is grandparent
        company_name = pdf_path.parent.parent.name
    else:
        # Company is parent directory
        company_name = pdf_path.parent.name
    
    # Clean up company name
    company_name = company_name.replace('-', ' ').replace('_', ' ').strip()
    
    return company_name, year


def process_single_pdf(pdf_path: Path) -> Dict[str, Any]:
    """
    Process a single PDF file through the entire pipeline.
    
    Args:
        pdf_path: Path to the PDF file
        
    Returns:
        Dictionary with processing results
    """
    logger = logging.getLogger(__name__)
    logger.info(f"\n{'='*80}")
    logger.info(f"Processing: {pdf_path.name}")
    logger.info(f"{'='*80}")
    
    result = {
        "pdf_path": str(pdf_path),
        "status": "failed",
        "error": None
    }
    
    try:
        # Parse company and year
        company_name, year = parse_company_year(pdf_path)
        logger.info(f"Company: {company_name}, Year: {year}")
        
        result["company"] = company_name
        result["year"] = year
        
        # Check if already processed (idempotency)
        from config.config import OUTPUT_DIR
        output_dir = OUTPUT_DIR / company_name / year
        report_json = output_dir / "report.json"
        if report_json.exists():
            logger.info(f"âœ“ Skipping - already processed")
            result["status"] = "skipped"
            result["reason"] = "already_processed"
            return result
        
        # Step 1: Get PDF info
        logger.info("Step 1/6: Getting PDF information...")
        pdf_info = get_pdf_info(pdf_path)
        logger.info(f"  Pages: {pdf_info.get('pages', 'N/A')}, "
                   f"Size: {pdf_info.get('file_size_mb', 0):.2f} MB")
        
        # Step 2: Detect PDF type
        logger.info("Step 2/6: Detecting PDF type...")
        try:
            pdf_type, type_metadata = detect_pdf_type(pdf_path)
            logger.info(f"  Type: {pdf_type.upper()}")
        except Exception as e:
            logger.error(f"  Error detecting PDF type: {e}")
            result["status"] = "failed"
            result["error"] = f"pdf_type_detection_failed: {str(e)}"
            result["step"] = "detect_pdf_type"
            return result
        
        # Step 3: Extract text (with error handling)
        logger.info("Step 3/6: Extracting text...")
        try:
            pages, extraction_stats = extract_text(pdf_path, pdf_type)
            logger.info(f"  Extracted {len(pages)} pages")
        except Exception as e:
            logger.error(f"  Error extracting text: {e}")
            result["status"] = "failed"
            result["error"] = str(e)
            result["step"] = "extract_text"
            return result
        logger.info(f"  Content coverage: {extraction_stats['extraction_coverage']:.1f}%")
        
        # Step 4: Clean text (minimal cleaning to preserve layout)
        logger.info("Step 4/5: Cleaning text...")
        cleaned_pages = clean_pages(pages)
        total_chars = sum(p.char_count for p in cleaned_pages)
        logger.info(f"  Cleaned {total_chars:,} characters")
        
        # Step 5: Export to DOCX and JSON
        logger.info("Step 5/6: Exporting to DOCX and JSON...")
        
        # Create output directory
        output_path = create_output_directory(company_name, year)
        
        # Export DOCX with page-by-page content
        docx_path = export_to_docx(cleaned_pages, [], output_path, company_name, year)
        
        # Also export full report as hierarchical JSON
        logger.info("  Building hierarchical structure for full report...")
        full_text = "\n\n".join([p.text for p in cleaned_pages])
        
        # Build hierarchy for full report (use 'mdna' type as default for business reports)
        hierarchy_builder = SectionHierarchyBuilder(section_type='mdna')
        full_report_hierarchy = hierarchy_builder.build_section_hierarchy(
            text=full_text,
            company=company_name,
            year=year,
            section_name="Annual Report",
            start_page=1,
            end_page=len(cleaned_pages),
            confidence=1.0
        )
        
        # Export full report JSON
        json_report_path = output_path / "report.json"
        hierarchy_builder.export_section_json(full_report_hierarchy, json_report_path)
        logger.info(f"  Exported full report JSON to {json_report_path}")
        
        # Calculate extraction completeness with enhanced quality assessment
        pdf_total_pages = pdf_info.get('pages', len(cleaned_pages))
        completeness_pct = (extraction_stats['pages_with_content'] / pdf_total_pages * 100) if pdf_total_pages > 0 else 0
        
        # Determine overall extraction quality based on multiple factors
        quality_dist = extraction_stats.get('page_quality_distribution', {})
        good_pages = quality_dist.get('good_content_pages', 0)
        empty_pages = quality_dist.get('empty_pages', 0)
        
        # Quality score: weighted by good pages and penalized by empty pages
        quality_score = (good_pages / pdf_total_pages * 100) if pdf_total_pages > 0 else 0
        
        if completeness_pct >= 95 and empty_pages <= pdf_total_pages * 0.05:
            quality_rating = "excellent"
        elif completeness_pct >= 80 and empty_pages <= pdf_total_pages * 0.1:
            quality_rating = "good"
        elif completeness_pct >= 60:
            quality_rating = "fair"
        else:
            quality_rating = "poor"
        
        # Create detailed metadata
        metadata = {
            "company": company_name,
            "year": year,
            "processing_date": datetime.now().isoformat(),
            "pdf_info": pdf_info,
            "pdf_type": pdf_type,
            "extraction_summary": {
                "total_pages_in_pdf": pdf_total_pages,
                "pages_processed": len(cleaned_pages),
                "pages_with_content": extraction_stats['pages_with_content'],
                "total_characters": total_chars,
                "avg_chars_per_page": round(extraction_stats['avg_chars_per_page'], 2),
                "extraction_method": pdf_type,
                "extraction_coverage_percent": round(completeness_pct, 2),
                "extraction_quality": quality_rating,
                "quality_score": round(quality_score, 2)
            },
            "detailed_metrics": {
                "page_quality_distribution": extraction_stats.get('page_quality_distribution', {}),
                "character_statistics": extraction_stats.get('character_statistics', {}),
                "potential_issues": extraction_stats.get('potential_issues', {})
            }
        }
        
        json_path = output_path / "metadata.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        # Step 6: Extract MD&A and Letter to Stakeholders sections
        logger.info("Step 6/6: Extracting narrative sections...")
        try:
            section_files = extract_sections_from_pdf(
                pdf_path=pdf_path,
                pages=pages,  # Use original pages with extraction stats
                output_dir=output_path / "sections",
                company_name=company_name,
                year=year
            )
            logger.info(f"  Extracted {len(section_files)} section files")
        except Exception as e:
            logger.warning(f"  Section extraction failed: {e}")
            section_files = {}
        
        result["output_directory"] = str(output_path)
        result["docx"] = str(docx_path)
        result["metadata"] = str(json_path)
        result["report_json"] = str(json_report_path)
        result["files_created"] = [str(docx_path), str(json_path), str(json_report_path)]
        result["status"] = "success"
        
        logger.info(f"Successfully processed {pdf_path.name}")
        
    except Exception as e:
        logger.error(f"Error processing {pdf_path.name}: {e}", exc_info=True)
        result["error"] = str(e)
        result["status"] = "failed"
    
    return result


def find_all_pdfs(data_dir: Path) -> List[Path]:
    """
    Find all PDF files in the data directory.
    
    Args:
        data_dir: Root data directory
        
    Returns:
        List of paths to PDF files
    """
    pdf_files = list(data_dir.rglob("*.pdf"))
    return sorted(pdf_files)


def main():
    """Main entry point for the PDF processing pipeline."""
    
    # Setup logging
    log_file = LOGS_DIR / f"pipeline_{logging.Formatter().formatTime(logging.LogRecord('', 0, '', 0, '', (), None), '%Y%m%d_%H%M%S')}.log"
    setup_logging(log_file)
    
    logger = logging.getLogger(__name__)
    logger.info("Starting PDF Processing Pipeline")
    logger.info(f"Data directory: {DATA_DIR}")
    logger.info(f"Log file: {log_file}")
    
    # Find all PDFs
    logger.info("Scanning for PDF files...")
    pdf_files = find_all_pdfs(DATA_DIR)
    
    if not pdf_files:
        logger.error(f"No PDF files found in {DATA_DIR}")
        logger.info("Please add PDF files to the data directory")
        return
    
    logger.info(f"Found {len(pdf_files)} PDF file(s) to process")
    
    # Process each PDF
    results = []
    
    for pdf_path in tqdm(pdf_files, desc="Processing PDFs", unit="file"):
        result = process_single_pdf(pdf_path)
        results.append(result)
    
    # Generate summary
    logger.info("\n" + "="*80)
    logger.info("PROCESSING SUMMARY")
    logger.info("="*80)
    
    successful = sum(1 for r in results if r["status"] == "success")
    failed = sum(1 for r in results if r["status"] == "failed")
    
    logger.info(f"Total PDFs processed: {len(results)}")
    logger.info(f"Successful: {successful}")
    logger.info(f"Failed: {failed}")
    
    if failed > 0:
        logger.info("\nFailed files:")
        for r in results:
            if r["status"] == "failed":
                logger.info(f"  - {r['pdf_path']}: {r.get('error', 'Unknown error')}")
    
    # Save summary report
    summary_path = create_summary_report(results)
    logger.info(f"\nSummary report saved to: {summary_path}")
    
    logger.info("\nPipeline completed!")


if __name__ == "__main__":
    main()
